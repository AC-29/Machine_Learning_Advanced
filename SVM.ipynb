{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "#some data generation to test my svm\n",
    "\n",
    "np.random.seed(100)\n",
    "firstCluster=np.array(0.35*np.random.randn(10 , 2) + [1.5 ,0.5])\n",
    "secondCluster=np.array(0.35*np.random.randn(10 , 2) + [-1.5,0.5])\n",
    "classA = np.concatenate((firstCluster,secondCluster))\n",
    "classB = 0.35*np.random.randn(20 , 2) + [0.0 , -1.5]\n",
    "inputs = np.concatenate( ( classA , classB ) )\n",
    "targets= np.concatenate ((np.ones(classA.shape[0]) , -np.ones(classB.shape[0])))\n",
    "N = inputs . shape [ 0 ] # Number of rows ( samples )\n",
    "start=np.zeros(N)\n",
    "permute=list ( range (N) )\n",
    "random.shuffle ( permute )\n",
    "inputs = inputs [ permute , : ]\n",
    "targets = targets [ permute ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Kernel:\n",
    "    \n",
    "    def __init__(self,kern_type='linear'):\n",
    "        self.tag = kern_type\n",
    "        self.sigma = 1\n",
    "        self.p = 2\n",
    "        if self.tag not in ['linear,rbf,polynomial']:\n",
    "            print('Careful')\n",
    "        \n",
    "    def set_sigma(self,sig):\n",
    "        self.sigma = sig\n",
    "    \n",
    "    \n",
    "    def set_p(self,power):\n",
    "        self.p = power \n",
    "    \n",
    "    \n",
    "    def value(self,x,y):\n",
    "        if (self.tag=='linear'):\n",
    "            return np.dot(x,y)\n",
    "        elif (self.tag=='rbf'):\n",
    "            return  np.exp(-(1/(2*self.sigma**2))*np.linalg.norm(x-y)**2)\n",
    "        else:\n",
    "            return (np.dot(x,y)+1)**self.p\n",
    "        \n",
    "    def rbf_kernel(self,x,y,sigma):\n",
    "        return np.exp(-(1/(2*sigma**2))*np.linalg.norm(x-y)**2)\n",
    "\n",
    "    def linear_kernel(x,y):\n",
    "        return np.dot(x,y)\n",
    "\n",
    "    def polynomial_kernel(self,x,y,p):\n",
    "        return (np.dot(x,y)+1)**p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method to generate gaussian data from two classes\n",
    "\n",
    "def generateGaussianData(seed,mean1,sigma1,mean2,sigma2,N):\n",
    "    np.random.seed(seed)\n",
    "    classA = np.random.multivariate_normal(mean1,sigma1,int(N/2))\n",
    "    classB = np.random.multivariate_normal(mean2,sigma2,int(N/2))\n",
    "    inputs = np.concatenate( ( classA , classB ) )\n",
    "    targets= np.concatenate ((np.ones(classA.shape[0]) , -np.ones(classB.shape[0])))\n",
    "    start=np.zeros(N)\n",
    "    permute=list ( range (N) )\n",
    "    random.shuffle ( permute )\n",
    "    inputs = inputs [ permute , : ]\n",
    "    targets = targets [ permute ]\n",
    "    return targets,inputs,classA,classB\n",
    "mean1=np.array([2.5,0.5])\n",
    "mean2=np.array([0.0,-0.5])\n",
    "sigma1=np.array([[0.35,0],[0,0.35]])\n",
    "sigma2=np.array([[0.35,0],[0,0.35]])\n",
    "targ, inp, clA, clB = generateGaussianData(1,mean1,sigma1,mean2,sigma2,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Careful\n",
      "Careful\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SVM:\n",
    "    \n",
    "    def __init__(self,kernel='linear'):\n",
    "        self.kernel = Kernel(kernel)\n",
    "        self.a=[] #alphas of dual svm\n",
    "        self.b=0 #constant b\n",
    "        self.targets=[] #target variables +-1\n",
    "        self.p=[] #our kernel Matrix t_i * t_j * K(x_i,x_j)\n",
    "        self.inputs=[] #our inputs x_i\n",
    "        self.C=0 # slack C (if we want to run SVM with slack)\n",
    "        self.K=[]\n",
    "\n",
    "    def createMatrix(self,targets,inputs): #Generate our training data matrix (that takes into account our inputs)\n",
    "        N = targets.shape[0]\n",
    "        result=np.zeros(shape=(N,N))\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                result[i][j]=targets[i]*targets[j]*self.kernel.value(inputs[i],inputs[j])\n",
    "        return result\n",
    "\n",
    "    def createKernelMatrix(self,inputs): #Generate Kernel Matrix on labeled and unlabened data (useful for semi-supervised)\n",
    "        N = targets.shape[0]\n",
    "        result=np.zeros(shape=(N,N))\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                result[i][j]=self.kernel.value(inputs[i],inputs[j])\n",
    "        self.K = result\n",
    "        return result\n",
    "\n",
    "    def indicator(self,x): #indicator function. t[x] = 1 iff indicator(x) >=0\n",
    "        number = 0\n",
    "        marginIndex = np.nonzero(self.a>0.00001)\n",
    "        for i in marginIndex[0]:\n",
    "            number += self.a[i] * self.targets[i] * self.kernel.value(x, self.inputs[i])\n",
    "        number -= self.b\n",
    "        return number\n",
    "\n",
    "    def fit_on_data(self,inputs,targets, C=0):\n",
    "        N = targets.shape[0]\n",
    "        self.C=C\n",
    "        if (C==0): #define bounds based on whether or not we use slack\n",
    "\n",
    "            bounds = [(0, None) for b in range(N)]\n",
    "        else:\n",
    "            bounds = [(0, C) for b in range(N)]\n",
    "\n",
    "        p=self.createMatrix(targets,inputs) # get the kernel-target matrix\n",
    "        self.p=p\n",
    "        self.targets=targets\n",
    "        self.inputs=inputs\n",
    "        def objective(a): #objective function that is to be minimized\n",
    "            result = np.dot(a,np.dot(self.p,a))/2 - np.sum(a)\n",
    "            return result\n",
    "        def zerofun(a): #constraints\n",
    "            return np.dot(a,self.targets)\n",
    "        #minimize the function\n",
    "        initial_a=np.zeros(N)\n",
    "        const = {'type': 'eq', 'fun': zerofun}\n",
    "        ret = minimize(objective, initial_a,\n",
    "        bounds=bounds,constraints=const)\n",
    "\n",
    "        alpha = ret['x'] #return alphas\n",
    "        self.a=alpha\n",
    "\n",
    "        #calculating b\n",
    "\n",
    "        marginIndex=np.nonzero(alpha>0.00001) #give me the 'meaningful' alphas\n",
    "        #mI : give me the indeces corresponding to support vectors\n",
    "        if C>0:\n",
    "            mI=np.nonzero((alpha>0.00001) & (alpha < C))\n",
    "        else:\n",
    "            mI=np.nonzero((alpha>0.00001))\n",
    "        marginX=inputs[mI[0]][0] #pick one SV\n",
    "        targetX=targets[mI[0]][0] #and its target\n",
    "        b=0\n",
    "        for i in marginIndex[0]:\n",
    "            b+= alpha[i]*targets[i]*self.kernel.value(marginX,inputs[i])\n",
    "        b-=targetX\n",
    "        self.b=b\n",
    "        \n",
    "    def plot_2D_SVM(self,classA,classB):\n",
    "        #select the support vectors\n",
    "        if self.C>0:\n",
    "            marginIndex = np.nonzero((self.a>0.00001) & (self.a < self.C))\n",
    "        else:\n",
    "            marginIndex = np.nonzero((self.a>0.00001))\n",
    "        #plot the data points per class\n",
    "        plt.plot([p[0] for p in classA],\n",
    "                 [p[1] for p in classA], 'b.', label='Class A')\n",
    "        plt.plot([p[0] for p in classB],\n",
    "                 [p[1] for p in classB],'r.',label='Class B')\n",
    "        plt.plot([p[0] for p in self.inputs[marginIndex[0][self.targets[marginIndex[0]]==1]]],\n",
    "                 [p[1] for p in self.inputs[marginIndex[0][self.targets[marginIndex[0]]==1]]], 'b*', label='class A SV')\n",
    "        plt.plot([p[0] for p in self.inputs[marginIndex[0][self.targets[marginIndex[0]]==-1]]],\n",
    "                 [p[1] for p in self.inputs[marginIndex[0][self.targets[marginIndex[0]]==-1]]], 'r*', label='class B SV')\n",
    "        plt.axis('equal')\n",
    "        plt.xlabel('x feature')\n",
    "        plt.ylabel('y feature')\n",
    "        title = 'SVM, ' + self.kernel.tag + ' Kernel, C=' + str(self.C)\n",
    "        plt.title(title)\n",
    "\n",
    "        xgrid=np.linspace(-5,5)\n",
    "        ygrid=np.linspace(-4,4)\n",
    "\n",
    "        grid = np.array([[self.indicator([x,y]) for x in xgrid] for y in ygrid]) #prepare for contour on indicator\n",
    "        \n",
    "        \n",
    "        #contour =+1 upper border, =0 center (classification border), =-1 below border\n",
    "        plt.contour(xgrid,ygrid,grid,\n",
    "                   (-1.0,0.0,1.0),\n",
    "                   colors=('red','black','blue'),\n",
    "                   linewidths=(1,3,1))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self,x): #predict base on your indicator function\n",
    "        N=x.shape[0]\n",
    "        result = np.ones(N)\n",
    "        for i in range(N):\n",
    "            \n",
    "            if(self.indicator(x[i])<0):\n",
    "                result[i] = -1\n",
    "        return result\n",
    "    \n",
    "    def accuracy(self,x,t): #compute the ratio (successfully classified)/total\n",
    "        N = x.shape[0]\n",
    "        predicted = self.predict(x)\n",
    "        correct = 0\n",
    "        for i in range(N):\n",
    "            if predicted[i] == t[i]:\n",
    "                correct+=1\n",
    "        return correct/N\n",
    "\n",
    "#creating an SVM kernel based on my class\n",
    "svm = SVM(kernel='linear')\n",
    "svm.fit_on_data(inp,targ,C=1)\n",
    "alph = svm.a\n",
    "bet = svm.b\n",
    "kern = Kernel()\n",
    "\n",
    "svm.plot_2D_SVM(clA,clB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom svm: 0.654\n",
      "sklearn svm: 0.654\n"
     ]
    }
   ],
   "source": [
    "#accuracy of our svm\n",
    "x=np.array(0.35*np.random.randn(1000 , 2) + [1.5 ,0.5])\n",
    "t=np.ones(x.shape[0])\n",
    "print('custom svm:',svm.accuracy(x,t))\n",
    "from sklearn import svm as SVM\n",
    "\n",
    "clf = SVM.SVC(kernel = 'linear', C = 1)\n",
    "clf.fit(inp,targ)\n",
    "print('sklearn svm:',clf.score(x,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# time to import the data!\n",
    "df_usps_train = pd.read_csv('zip.train',delim_whitespace=True,header=None)\n",
    "df_usps_test = pd.read_csv('zip.test',delim_whitespace=True,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1    2    3      4      5      6      7      8      9    ...    247  \\\n",
      "0  6.0 -1.0 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -0.631  0.862  ...  0.304   \n",
      "1  5.0 -1.0 -1.0 -1.0 -0.813 -0.671 -0.809 -0.887 -0.671 -0.853  ... -0.671   \n",
      "2  4.0 -1.0 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  ... -1.000   \n",
      "3  7.0 -1.0 -1.0 -1.0 -1.000 -1.000 -0.273  0.684  0.960  0.450  ... -0.318   \n",
      "4  3.0 -1.0 -1.0 -1.0 -1.000 -1.000 -0.928 -0.204  0.751  0.466  ...  0.466   \n",
      "\n",
      "     248    249    250    251    252    253    254    255  256  \n",
      "0  0.823  1.000  0.482 -0.474 -0.991 -1.000 -1.000 -1.000 -1.0  \n",
      "1 -0.671 -0.033  0.761  0.762  0.126 -0.095 -0.671 -0.828 -1.0  \n",
      "2 -1.000 -1.000 -0.109  1.000 -0.179 -1.000 -1.000 -1.000 -1.0  \n",
      "3  1.000  0.536 -0.987 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "4  0.639  1.000  1.000  0.791  0.439 -0.199 -0.883 -1.000 -1.0  \n",
      "\n",
      "[5 rows x 257 columns]\n",
      "(7291, 257)\n",
      "   0    1    2    3      4    5      6      7      8      9    ...    247  \\\n",
      "0    9 -1.0 -1.0 -1.0 -1.000 -1.0 -0.948 -0.561  0.148  0.384  ... -1.000   \n",
      "1    6 -1.0 -1.0 -1.0 -1.000 -1.0 -1.000 -1.000 -1.000 -1.000  ... -1.000   \n",
      "2    3 -1.0 -1.0 -1.0 -0.593  0.7  1.000  1.000  1.000  1.000  ...  1.000   \n",
      "3    6 -1.0 -1.0 -1.0 -1.000 -1.0 -1.000 -1.000 -1.000 -1.000  ... -1.000   \n",
      "4    6 -1.0 -1.0 -1.0 -1.000 -1.0 -1.000 -1.000 -0.858 -0.106  ...  0.901   \n",
      "\n",
      "     248    249    250    251    252  253  254  255  256  \n",
      "0 -0.908  0.430  0.622 -0.973 -1.000 -1.0 -1.0 -1.0 -1.0  \n",
      "1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0 -1.0 -1.0 -1.0  \n",
      "2  0.717  0.333  0.162 -0.393 -1.000 -1.0 -1.0 -1.0 -1.0  \n",
      "3 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0 -1.0 -1.0 -1.0  \n",
      "4  0.901  0.901  0.290 -0.369 -0.867 -1.0 -1.0 -1.0 -1.0  \n",
      "\n",
      "[5 rows x 257 columns]\n",
      "(2007, 257)\n"
     ]
    }
   ],
   "source": [
    "#see the data a bit\n",
    "print(df_usps_train.head())\n",
    "print(df_usps_train.shape)\n",
    "\n",
    "print(df_usps_test.head())\n",
    "print(df_usps_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0    1    2    3      4      5      6      7      8      9    ...  \\\n",
      "0       1 -1.0 -1.0 -1.0 -1.000 -1.000 -0.948 -0.561  0.148  0.384  ...   \n",
      "1       1 -1.0 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  ...   \n",
      "2      -1 -1.0 -1.0 -1.0 -0.593  0.700  1.000  1.000  1.000  1.000  ...   \n",
      "3       1 -1.0 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  ...   \n",
      "4       1 -1.0 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -0.858 -0.106  ...   \n",
      "...   ...  ...  ...  ...    ...    ...    ...    ...    ...    ...  ...   \n",
      "2002   -1 -1.0 -1.0 -1.0 -1.000 -1.000 -0.417  0.814  1.000  0.775  ...   \n",
      "2003    1 -1.0 -1.0 -1.0 -1.000 -1.000 -1.000 -0.855  0.209  0.941  ...   \n",
      "2004   -1 -1.0 -1.0 -1.0 -1.000 -0.031  0.752 -0.431 -1.000 -1.000  ...   \n",
      "2005   -1 -1.0 -1.0 -1.0 -1.000 -0.534  0.578  1.000  0.301 -0.328  ...   \n",
      "2006   -1 -1.0 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000  0.399  0.949  ...   \n",
      "\n",
      "        247    248    249    250    251    252  253  254  255  256  \n",
      "0    -1.000 -0.908  0.430  0.622 -0.973 -1.000 -1.0 -1.0 -1.0 -1.0  \n",
      "1    -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0 -1.0 -1.0 -1.0  \n",
      "2     1.000  0.717  0.333  0.162 -0.393 -1.000 -1.0 -1.0 -1.0 -1.0  \n",
      "3    -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0 -1.0 -1.0 -1.0  \n",
      "4     0.901  0.901  0.901  0.290 -0.369 -0.867 -1.0 -1.0 -1.0 -1.0  \n",
      "...     ...    ...    ...    ...    ...    ...  ...  ...  ...  ...  \n",
      "2002  1.000  1.000  0.554  0.184 -0.484 -1.000 -1.0 -1.0 -1.0 -1.0  \n",
      "2003 -1.000  0.319  1.000  0.056 -1.000 -1.000 -1.0 -1.0 -1.0 -1.0  \n",
      "2004  1.000  0.928 -0.393 -1.000 -1.000 -1.000 -1.0 -1.0 -1.0 -1.0  \n",
      "2005  1.000  0.430  0.200 -0.061 -0.975 -1.000 -1.0 -1.0 -1.0 -1.0  \n",
      "2006 -1.000 -0.505  0.907  0.655 -0.950 -1.000 -1.0 -1.0 -1.0 -1.0  \n",
      "\n",
      "[2007 rows x 257 columns]\n"
     ]
    }
   ],
   "source": [
    "#train on USPS\n",
    "#label data as -1 iff digit <=4, else 1\n",
    "df_usps_modified = df_usps_test.copy()\n",
    "for i in range(df_usps_modified.shape[0]):\n",
    "    if df_usps_modified.loc[i,0] <=4:\n",
    "        df_usps_modified.loc[i,0] = -1\n",
    "    else:\n",
    "        df_usps_modified.loc[i,0] = 1\n",
    "\n",
    "        \n",
    "print(df_usps_modified)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0    1      2      3      4      5      6      7      8      9    ...  \\\n",
      "1830   -1 -1.0 -0.123  0.647 -0.377 -0.829 -0.976 -1.000 -1.000 -1.000  ...   \n",
      "36      1 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  ...   \n",
      "803     1 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -0.324  0.909  0.790  ...   \n",
      "1903    1 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -0.968 -0.144  0.792  ...   \n",
      "1332   -1 -1.0 -1.000 -1.000 -1.000 -0.805  0.154  0.914  1.000  0.778  ...   \n",
      "...   ...  ...    ...    ...    ...    ...    ...    ...    ...    ...  ...   \n",
      "1032    1 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  ...   \n",
      "1905   -1 -1.0 -1.000 -1.000 -1.000 -1.000  0.242  0.723 -0.521  0.135  ...   \n",
      "1949   -1 -1.0 -1.000 -1.000 -0.051  0.131 -1.000 -1.000 -1.000 -1.000  ...   \n",
      "1520   -1 -1.0 -1.000 -1.000 -1.000 -0.838  0.192 -0.912 -1.000 -1.000  ...   \n",
      "733    -1 -1.0 -1.000 -1.000 -1.000  0.144  0.162 -1.000 -1.000 -1.000  ...   \n",
      "\n",
      "        247    248    249    250    251    252    253  254  255  256  \n",
      "1830  1.000  0.635 -0.007 -0.752 -1.000 -1.000 -1.000 -1.0 -1.0 -1.0  \n",
      "36   -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0 -1.0 -1.0  \n",
      "803   0.714  0.284 -0.899 -1.000 -1.000 -1.000 -1.000 -1.0 -1.0 -1.0  \n",
      "1903 -0.994  0.390  0.795 -0.886 -1.000 -1.000 -1.000 -1.0 -1.0 -1.0  \n",
      "1332  1.000  0.673  0.110 -0.660 -1.000 -1.000 -1.000 -1.0 -1.0 -1.0  \n",
      "...     ...    ...    ...    ...    ...    ...    ...  ...  ...  ...  \n",
      "1032 -0.717  0.207  1.000  1.000  0.634 -0.213 -0.996 -1.0 -1.0 -1.0  \n",
      "1905  0.391  1.000  1.000  0.534 -0.008 -0.721 -1.000 -1.0 -1.0 -1.0  \n",
      "1949 -1.000 -1.000 -0.126  0.125 -1.000 -1.000 -1.000 -1.0 -1.0 -1.0  \n",
      "1520  0.249 -0.885 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0 -1.0 -1.0  \n",
      "733  -1.000 -1.000 -0.660  0.756 -0.135 -1.000 -1.000 -1.0 -1.0 -1.0  \n",
      "\n",
      "[2000 rows x 257 columns]\n"
     ]
    }
   ],
   "source": [
    "seed_value=10\n",
    "#sample 2000 from them\n",
    "df_usps_sample = df_usps_modified.sample(2000,random_state=seed_value)\n",
    "\n",
    "print(df_usps_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method of partitioning data fram in 50*40\n",
    "def partition_dataframe(df):\n",
    "    result=[]\n",
    "    df_to_sample_from = df.copy()\n",
    "    for i in range(0,50):\n",
    "        train = df_to_sample_from.sample(40,random_state=seed_value)\n",
    "        remaining = df_to_sample_from.drop(train.index)\n",
    "        df_to_sample_from = remaining\n",
    "        result.append(train)\n",
    "    return result\n",
    "partition = partition_dataframe(df_usps_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0      1      2      3      4      5      6      7      8      9    ...  \\\n",
      "1721    1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  ...   \n",
      "1732   -1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.675  0.674  0.504  ...   \n",
      "1213    1 -0.848  0.043  0.662  0.662  0.662  0.662  0.662  0.662  0.279  ...   \n",
      "1889    1 -1.000 -1.000 -1.000 -0.839 -0.561 -0.561 -0.561  0.082  0.998  ...   \n",
      "374    -1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.956  0.799  0.340  ...   \n",
      "1091   -1 -1.000 -1.000 -1.000 -1.000 -1.000 -0.213  1.000  0.661 -0.264  ...   \n",
      "1266    1 -1.000 -1.000 -0.995  0.163  0.890  1.000  1.000  0.820  0.524  ...   \n",
      "337    -1 -1.000 -1.000 -1.000 -1.000 -1.000 -0.890 -0.664 -1.000 -1.000  ...   \n",
      "935    -1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  ...   \n",
      "1614    1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.621  0.602  ...   \n",
      "656    -1 -1.000 -1.000 -1.000 -0.926  0.383  0.433  0.604  0.444  0.384  ...   \n",
      "1994   -1 -1.000 -1.000 -1.000 -0.939  0.536  1.000  1.000  1.000  0.871  ...   \n",
      "1064   -1 -1.000 -1.000 -1.000 -1.000 -1.000 -0.366  0.643  0.655 -0.087  ...   \n",
      "1046    1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  0.121  ...   \n",
      "1780   -1 -1.000 -1.000 -1.000 -1.000 -0.975  0.375  0.309  0.057  0.374  ...   \n",
      "1908   -1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  ...   \n",
      "513    -1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.999  0.321 -0.225  ...   \n",
      "560     1 -1.000 -1.000 -0.122  0.840  0.151 -0.721 -0.888 -1.000 -1.000  ...   \n",
      "911    -1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  ...   \n",
      "1543    1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.892  ...   \n",
      "1649   -1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.281  0.854  ...   \n",
      "1741   -1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.945  0.663 -0.632  ...   \n",
      "479    -1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  ...   \n",
      "661    -1 -1.000 -1.000 -1.000 -1.000 -0.486  0.635  1.000  1.000  1.000  ...   \n",
      "1274   -1 -1.000 -1.000 -1.000 -1.000 -0.503  0.686  1.000  0.979 -0.054  ...   \n",
      "1260    1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.326  1.000  ...   \n",
      "408    -1 -1.000 -1.000 -1.000 -1.000 -1.000 -0.942  0.597  0.329 -0.574  ...   \n",
      "973     1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.514  0.529  0.993  ...   \n",
      "299     1 -0.810  0.944  0.142  0.387  0.438  0.067 -0.123 -0.123 -0.273  ...   \n",
      "977     1 -1.000 -1.000 -1.000 -0.811 -0.013  0.604  1.000  0.657 -0.850  ...   \n",
      "150     1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.955  0.613  ...   \n",
      "1764   -1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.009  0.877 -0.119  ...   \n",
      "1264    1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.515  0.517  ...   \n",
      "272    -1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.925 -0.487 -0.778  ...   \n",
      "177     1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.967 -0.645  ...   \n",
      "1116   -1 -1.000 -1.000 -1.000 -1.000 -0.970 -0.134  0.654  0.135  0.135  ...   \n",
      "604     1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  ...   \n",
      "1956    1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.938  0.134  0.827  ...   \n",
      "1923    1 -1.000 -0.362  0.200  0.571  1.000  1.000  1.000  0.948  0.200  ...   \n",
      "529    -1 -1.000 -1.000 -1.000 -1.000 -0.981  0.409  1.000  0.453 -0.449  ...   \n",
      "\n",
      "        247    248    249    250    251    252    253    254    255  256  \n",
      "1721 -1.000 -0.975 -0.364  0.002  0.571  0.530  0.319  0.012 -0.942 -1.0  \n",
      "1732 -0.696  0.934 -0.385 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "1213 -0.147 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "1889  1.000  1.000  0.748  0.126 -0.693 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "374  -1.000 -0.436  0.828  0.257 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "1091  0.550 -0.019 -0.745 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "1266  0.981  0.453 -0.625 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "337  -0.618 -0.618 -0.807 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "935  -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "1614 -0.726  0.808 -0.731 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "656   0.679  0.497  0.845  0.306 -0.538 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "1994  1.000  1.000  1.000  1.000  1.000  0.748  0.026 -0.827 -1.000 -1.0  \n",
      "1064  0.944  1.000  0.922  0.448 -0.027 -0.271 -0.847 -1.000 -1.000 -1.0  \n",
      "1046 -1.000 -0.765  0.171  0.654 -0.960 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "1780  0.500  0.620 -0.999 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "1908 -1.000 -0.266  0.855 -0.959 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "513  -1.000  0.642 -0.276 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "560   0.958  0.702 -0.767 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "911  -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "1543 -1.000 -1.000  0.505 -0.401 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "1649  0.100  0.956  1.000  0.967 -0.046 -0.991 -1.000 -1.000 -1.000 -1.0  \n",
      "1741 -0.913  0.683 -0.919 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "479  -0.994 -0.569  0.056  0.384  0.811  0.595 -0.568 -1.000 -1.000 -1.0  \n",
      "661   1.000  1.000  0.991  0.201  0.200 -0.320 -0.792 -1.000 -1.000 -1.0  \n",
      "1274  1.000  1.000  1.000  0.687  0.318 -0.430 -0.969 -1.000 -1.000 -1.0  \n",
      "1260 -0.817  0.175  0.737  0.310 -0.117 -0.884 -1.000 -1.000 -1.000 -1.0  \n",
      "408  -0.889  0.181  0.685 -0.084 -0.994 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "973   0.732  1.000  0.694  0.167 -0.483 -0.973 -1.000 -1.000 -1.000 -1.0  \n",
      "299   0.847  0.779 -0.757 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "977   0.879 -0.201 -0.992 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "150   0.803  0.525 -0.335 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "1764 -1.000 -0.923  0.133  0.983  0.448 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "1264  0.584  0.734  0.376 -0.827 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "272  -0.487 -0.693 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "177  -1.000 -0.935  0.158  0.784  1.000  0.514 -0.202 -1.000 -1.000 -1.0  \n",
      "1116  0.260  1.000  0.977 -0.164 -0.803 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "604  -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "1956 -0.992  0.377  0.397 -0.967 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "1923  0.080  1.000  0.840 -0.519 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "529   0.030  0.750  0.250 -0.376 -0.970 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "\n",
      "[40 rows x 257 columns]\n",
      "0.801530612244898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"svm = SVM(kernel='linear')\\nsvm.fit_on_data(train_data,train_targets,C=1)\\nprint(svm.accuracy(test_data,test_targets))\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(partition[0])\n",
    "#this is just testing the first case (where we take the first 40 as train)\n",
    "#Just to see accuracy\n",
    "train_data = partition[0].iloc[:,1:].to_numpy()\n",
    "train_targets = partition[0].iloc[:,0].to_numpy()\n",
    "df_test = df_usps_sample.drop(partition[0].index)\n",
    "test_data = df_test.iloc[:,1:].to_numpy()\n",
    "test_targets = df_test.iloc[:,0].to_numpy()\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='rbf',C=10,gamma='scale')\n",
    "clf.fit(train_data, train_targets)\n",
    "predicted = clf.predict(test_data)\n",
    "N = test_targets.shape[0]\n",
    "correct = 0\n",
    "for j in range(N):\n",
    "    if predicted[j] == test_targets[j]:\n",
    "        correct+=1\n",
    "print(correct/N)\n",
    "\n",
    "'''svm = SVM(kernel='linear')\n",
    "svm.fit_on_data(train_data,train_targets,C=1)\n",
    "print(svm.accuracy(test_data,test_targets))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for sigma:  8\n",
      "[0.80204082 0.77244898 0.77346939 0.75612245 0.77755102 0.8494898\n",
      " 0.80153061 0.78316327 0.80969388 0.75357143 0.80816327 0.75357143\n",
      " 0.78316327 0.80663265 0.76326531 0.73214286 0.76836735 0.81377551\n",
      " 0.80510204 0.74183673 0.74132653 0.75612245 0.70408163 0.73010204\n",
      " 0.77244898 0.78265306 0.7877551  0.76581633 0.79387755 0.7255102\n",
      " 0.80816327 0.77397959 0.74642857 0.78826531 0.76530612 0.78673469\n",
      " 0.74081633 0.7994898  0.76479592 0.73316327 0.75918367 0.78163265\n",
      " 0.76377551 0.77040816 0.77346939 0.75357143 0.80153061 0.79285714\n",
      " 0.7994898  0.79540816]\n",
      "mean: 0.774265306122449\n",
      "std: 0.027478960942157942\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.zeros(50)\n",
    "from sklearn import svm\n",
    "sigma=10\n",
    "gam = 1/(2*sigma**2)\n",
    "#do the partition\n",
    "for i in range(50):\n",
    "    #for each i, use partition[i] as train, and the rest as test\n",
    "    data_partition = partition[i].iloc[:,1:]\n",
    "    targets = partition[i].iloc[:,0].to_numpy()\n",
    "    inputs =  data_partition.to_numpy()\n",
    "    test_df = df_usps_sample.drop(partition[i].index) #'drop' simply ingores the indeces from particion [i], giving us the 'rest'\n",
    "    test_targets=test_df.iloc[:,0].to_numpy()\n",
    "    test_inputs =test_df.iloc[:,1:].to_numpy()\n",
    "    #svm = SVM(kernel='linear')\n",
    "    clf = svm.SVC(kernel='rbf', C=10,gamma=gam,class_weight='balanced')\n",
    "    clf.fit(inputs, targets)\n",
    "    #print(test_inputs)\n",
    "    #svm.fit_on_data(inputs,targets)\n",
    "    #accuracy[i] = svm.accuracy(test_inputs,test_targets)\n",
    "    #predicted = clf.predict(test_inputs)\n",
    "    #N = test_targets.shape[0]\n",
    "    #correct = 0\n",
    "    #for j in range(N):\n",
    "    #    if predicted[j] == test_targets[j]:\n",
    "    #        correct+=1\n",
    "    #accuracy[i]= correct/N\n",
    "    accuracy[i] = clf.score(test_inputs,test_targets)\n",
    "print('for sigma: ', s)\n",
    "print(accuracy)\n",
    "print('mean:', np.mean(accuracy))\n",
    "print('std:', np.std(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
