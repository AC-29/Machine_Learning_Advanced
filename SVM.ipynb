{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "#some data generation to test my svm\n",
    "\n",
    "np.random.seed(100)\n",
    "firstCluster=np.array(0.35*np.random.randn(10 , 2) + [1.5 ,0.5])\n",
    "secondCluster=np.array(0.35*np.random.randn(10 , 2) + [-1.5,0.5])\n",
    "classA = np.concatenate((firstCluster,secondCluster))\n",
    "classB = 0.35*np.random.randn(20 , 2) + [0.0 , -1.5]\n",
    "inputs = np.concatenate( ( classA , classB ) )\n",
    "targets= np.concatenate ((np.ones(classA.shape[0]) , -np.ones(classB.shape[0])))\n",
    "N = inputs . shape [ 0 ] # Number of rows ( samples )\n",
    "start=np.zeros(N)\n",
    "permute=list ( range (N) )\n",
    "random.shuffle ( permute )\n",
    "inputs = inputs [ permute , : ]\n",
    "targets = targets [ permute ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Kernel:\n",
    "    \n",
    "    def __init__(self,kern_type='linear'):\n",
    "        self.tag = kern_type\n",
    "        self.sigma = 1\n",
    "        self.p = 2\n",
    "        if self.tag not in ['linear,rbf,polynomial']:\n",
    "            print('Careful')\n",
    "        \n",
    "    def set_sigma(self,sig):\n",
    "        self.sigma = sig\n",
    "    \n",
    "    \n",
    "    def set_p(self,power):\n",
    "        self.p = power \n",
    "    \n",
    "    \n",
    "    def value(self,x,y):\n",
    "        if (self.tag=='linear'):\n",
    "            return np.dot(x,y)\n",
    "        elif (self.tag=='rbf'):\n",
    "            return  np.exp(-(1/(2*self.sigma**2))*np.linalg.norm(x-y)**2)\n",
    "        else:\n",
    "            return (np.dot(x,y)+1)**self.p\n",
    "        \n",
    "    def rbf_kernel(self,x,y,sigma):\n",
    "        return np.exp(-(1/(2*sigma**2))*np.linalg.norm(x-y)**2)\n",
    "\n",
    "    def linear_kernel(x,y):\n",
    "        return np.dot(x,y)\n",
    "\n",
    "    def polynomial_kernel(self,x,y,p):\n",
    "        return (np.dot(x,y)+1)**p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method to generate gaussian data from two classes\n",
    "\n",
    "def generateGaussianData(seed,mean1,sigma1,mean2,sigma2,N):\n",
    "    np.random.seed(seed)\n",
    "    classA = np.random.multivariate_normal(mean1,sigma1,int(N/2))\n",
    "    classB = np.random.multivariate_normal(mean2,sigma2,int(N/2))\n",
    "    inputs = np.concatenate( ( classA , classB ) )\n",
    "    targets= np.concatenate ((np.ones(classA.shape[0]) , -np.ones(classB.shape[0])))\n",
    "    start=np.zeros(N)\n",
    "    permute=list ( range (N) )\n",
    "    random.shuffle ( permute )\n",
    "    inputs = inputs [ permute , : ]\n",
    "    targets = targets [ permute ]\n",
    "    return targets,inputs,classA,classB\n",
    "mean1=np.array([2.5,0.5])\n",
    "mean2=np.array([0.0,-0.5])\n",
    "sigma1=np.array([[0.35,0],[0,0.35]])\n",
    "sigma2=np.array([[0.35,0],[0,0.35]])\n",
    "targ, inp, clA, clB = generateGaussianData(1,mean1,sigma1,mean2,sigma2,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Careful\n",
      "Careful\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SVM:\n",
    "    \n",
    "    def __init__(self,kernel='linear'):\n",
    "        self.kernel = Kernel(kernel)\n",
    "        self.a=[] #alphas of dual svm\n",
    "        self.b=0 #constant b\n",
    "        self.targets=[] #target variables +-1\n",
    "        self.p=[] #our kernel Matrix t_i * t_j * K(x_i,x_j)\n",
    "        self.inputs=[] #our inputs x_i\n",
    "        self.C=0 # slack C (if we want to run SVM with slack)\n",
    "        self.K=[]\n",
    "\n",
    "    def createMatrix(self,targets,inputs): #Generate our training data matrix (that takes into account our inputs)\n",
    "        N = targets.shape[0]\n",
    "        result=np.zeros(shape=(N,N))\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                result[i][j]=targets[i]*targets[j]*self.kernel.value(inputs[i],inputs[j])\n",
    "        return result\n",
    "\n",
    "    def createKernelMatrix(self,inputs): #Generate Kernel Matrix on labeled and unlabened data (useful for semi-supervised)\n",
    "        N = targets.shape[0]\n",
    "        result=np.zeros(shape=(N,N))\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                result[i][j]=self.kernel.value(inputs[i],inputs[j])\n",
    "        self.K = result\n",
    "        return result\n",
    "\n",
    "    def indicator(self,x): #indicator function. t[x] = 1 iff indicator(x) >=0\n",
    "        number = 0\n",
    "        marginIndex = np.nonzero(self.a>0.00001)\n",
    "        for i in marginIndex[0]:\n",
    "            number += self.a[i] * self.targets[i] * self.kernel.value(x, self.inputs[i])\n",
    "        number -= self.b\n",
    "        return number\n",
    "\n",
    "    def fit_on_data(self,inputs,targets, C=0):\n",
    "        N = targets.shape[0]\n",
    "        self.C=C\n",
    "        if (C==0): #define bounds based on whether or not we use slack\n",
    "\n",
    "            bounds = [(0, None) for b in range(N)]\n",
    "        else:\n",
    "            bounds = [(0, C) for b in range(N)]\n",
    "\n",
    "        p=self.createMatrix(targets,inputs) # get the kernel-target matrix\n",
    "        self.p=p\n",
    "        self.targets=targets\n",
    "        self.inputs=inputs\n",
    "        def objective(a): #objective function that is to be minimized\n",
    "            result = np.dot(a,np.dot(self.p,a))/2 - np.sum(a)\n",
    "            return result\n",
    "        def zerofun(a): #constraints\n",
    "            return np.dot(a,self.targets)\n",
    "        #minimize the function\n",
    "        initial_a=np.zeros(N)\n",
    "        const = {'type': 'eq', 'fun': zerofun}\n",
    "        ret = minimize(objective, initial_a,\n",
    "        bounds=bounds,constraints=const)\n",
    "\n",
    "        alpha = ret['x'] #return alphas\n",
    "        self.a=alpha\n",
    "\n",
    "        #calculating b\n",
    "\n",
    "        marginIndex=np.nonzero(alpha>0.00001) #give me the 'meaningful' alphas\n",
    "        #mI : give me the indeces corresponding to support vectors\n",
    "        if C>0:\n",
    "            mI=np.nonzero((alpha>0.00001) & (alpha < C))\n",
    "        else:\n",
    "            mI=np.nonzero((alpha>0.00001))\n",
    "        marginX=inputs[mI[0]][0] #pick one SV\n",
    "        targetX=targets[mI[0]][0] #and its target\n",
    "        b=0\n",
    "        for i in marginIndex[0]:\n",
    "            b+= alpha[i]*targets[i]*self.kernel.value(marginX,inputs[i])\n",
    "        b-=targetX\n",
    "        self.b=b\n",
    "        \n",
    "    def plot_2D_SVM(self,classA,classB):\n",
    "        #select the support vectors\n",
    "        if self.C>0:\n",
    "            marginIndex = np.nonzero((self.a>0.00001) & (self.a < self.C))\n",
    "        else:\n",
    "            marginIndex = np.nonzero((self.a>0.00001))\n",
    "        #plot the data points per class\n",
    "        plt.plot([p[0] for p in classA],\n",
    "                 [p[1] for p in classA], 'b.', label='Class A')\n",
    "        plt.plot([p[0] for p in classB],\n",
    "                 [p[1] for p in classB],'r.',label='Class B')\n",
    "        plt.plot([p[0] for p in self.inputs[marginIndex[0][self.targets[marginIndex[0]]==1]]],\n",
    "                 [p[1] for p in self.inputs[marginIndex[0][self.targets[marginIndex[0]]==1]]], 'b*', label='class A SV')\n",
    "        plt.plot([p[0] for p in self.inputs[marginIndex[0][self.targets[marginIndex[0]]==-1]]],\n",
    "                 [p[1] for p in self.inputs[marginIndex[0][self.targets[marginIndex[0]]==-1]]], 'r*', label='class B SV')\n",
    "        plt.axis('equal')\n",
    "        plt.xlabel('x feature')\n",
    "        plt.ylabel('y feature')\n",
    "        title = 'SVM, ' + self.kernel.tag + ' Kernel, C=' + str(self.C)\n",
    "        plt.title(title)\n",
    "\n",
    "        xgrid=np.linspace(-5,5)\n",
    "        ygrid=np.linspace(-4,4)\n",
    "\n",
    "        grid = np.array([[self.indicator([x,y]) for x in xgrid] for y in ygrid]) #prepare for contour on indicator\n",
    "        \n",
    "        \n",
    "        #contour =+1 upper border, =0 center (classification border), =-1 below border\n",
    "        plt.contour(xgrid,ygrid,grid,\n",
    "                   (-1.0,0.0,1.0),\n",
    "                   colors=('red','black','blue'),\n",
    "                   linewidths=(1,3,1))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self,x): #predict base on your indicator function\n",
    "        N=x.shape[0]\n",
    "        result = np.ones(N)\n",
    "        for i in range(N):\n",
    "            \n",
    "            if(self.indicator(x[i])<0):\n",
    "                result[i] = -1\n",
    "        return result\n",
    "    \n",
    "    def accuracy(self,x,t): #compute the ratio (successfully classified)/total\n",
    "        N = x.shape[0]\n",
    "        predicted = self.predict(x)\n",
    "        correct = 0\n",
    "        for i in range(N):\n",
    "            if predicted[i] == t[i]:\n",
    "                correct+=1\n",
    "        return correct/N\n",
    "\n",
    "#creating an SVM kernel based on my class\n",
    "svm = SVM(kernel='linear')\n",
    "svm.fit_on_data(inp,targ,C=1)\n",
    "alph = svm.a\n",
    "bet = svm.b\n",
    "kern = Kernel()\n",
    "\n",
    "svm.plot_2D_SVM(clA,clB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom svm: 0.65\n",
      "sklearn svm: 0.65\n"
     ]
    }
   ],
   "source": [
    "#accuracy of our svm\n",
    "x=np.array(0.35*np.random.randn(1000 , 2) + [1.5 ,0.5])\n",
    "t=np.ones(x.shape[0])\n",
    "print('custom svm:',svm.accuracy(x,t))\n",
    "from sklearn import svm as SVM\n",
    "\n",
    "clf = SVM.SVC(kernel = 'linear', C = 1)\n",
    "clf.fit(inp,targ)\n",
    "print('sklearn svm:',clf.score(x,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# time to import the data!\n",
    "df_usps = pd.read_csv('zip.train',delim_whitespace=True,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1    2    3      4      5      6      7      8      9    ...    247  \\\n",
      "0  6.0 -1.0 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -0.631  0.862  ...  0.304   \n",
      "1  5.0 -1.0 -1.0 -1.0 -0.813 -0.671 -0.809 -0.887 -0.671 -0.853  ... -0.671   \n",
      "2  4.0 -1.0 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  ... -1.000   \n",
      "3  7.0 -1.0 -1.0 -1.0 -1.000 -1.000 -0.273  0.684  0.960  0.450  ... -0.318   \n",
      "4  3.0 -1.0 -1.0 -1.0 -1.000 -1.000 -0.928 -0.204  0.751  0.466  ...  0.466   \n",
      "\n",
      "     248    249    250    251    252    253    254    255  256  \n",
      "0  0.823  1.000  0.482 -0.474 -0.991 -1.000 -1.000 -1.000 -1.0  \n",
      "1 -0.671 -0.033  0.761  0.762  0.126 -0.095 -0.671 -0.828 -1.0  \n",
      "2 -1.000 -1.000 -0.109  1.000 -0.179 -1.000 -1.000 -1.000 -1.0  \n",
      "3  1.000  0.536 -0.987 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "4  0.639  1.000  1.000  0.791  0.439 -0.199 -0.883 -1.000 -1.0  \n",
      "\n",
      "[5 rows x 257 columns]\n",
      "(7291, 257)\n",
      "0      9.000\n",
      "1      1.638\n",
      "2      2.000\n",
      "3      2.000\n",
      "4      2.000\n",
      "       ...  \n",
      "252    2.000\n",
      "253    2.000\n",
      "254    2.000\n",
      "255    2.000\n",
      "256    1.592\n",
      "Length: 257, dtype: float64\n",
      "(array([], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "#see the data a bit\n",
    "print(df_usps.head())\n",
    "print(df_usps.shape)\n",
    "print(df_usps.max() - df_usps.min())\n",
    "print(np.where(df_usps.std()==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0    1    2    3      4      5      6      7      8      9    ...  \\\n",
      "0     1.0 -1.0 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -0.631  0.862  ...   \n",
      "1     1.0 -1.0 -1.0 -1.0 -0.813 -0.671 -0.809 -0.887 -0.671 -0.853  ...   \n",
      "2    -1.0 -1.0 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  ...   \n",
      "3     1.0 -1.0 -1.0 -1.0 -1.000 -1.000 -0.273  0.684  0.960  0.450  ...   \n",
      "4    -1.0 -1.0 -1.0 -1.0 -1.000 -1.000 -0.928 -0.204  0.751  0.466  ...   \n",
      "...   ...  ...  ...  ...    ...    ...    ...    ...    ...    ...  ...   \n",
      "7286 -1.0 -1.0 -1.0 -1.0 -0.988 -0.527 -0.208  0.620  1.000  0.467  ...   \n",
      "7287 -1.0 -1.0 -1.0 -1.0 -0.990  0.708  0.557  0.347 -0.107 -0.758  ...   \n",
      "7288 -1.0 -1.0 -1.0 -1.0 -0.783 -0.984 -0.827  0.068  1.000  1.000  ...   \n",
      "7289 -1.0 -1.0 -1.0 -1.0 -1.000 -1.000 -0.549  0.463  0.999  0.999  ...   \n",
      "7290 -1.0 -1.0 -1.0 -1.0 -1.000 -1.000 -0.108  1.000  0.616 -0.867  ...   \n",
      "\n",
      "        247    248    249    250    251    252    253    254    255  256  \n",
      "0     0.304  0.823  1.000  0.482 -0.474 -0.991 -1.000 -1.000 -1.000 -1.0  \n",
      "1    -0.671 -0.671 -0.033  0.761  0.762  0.126 -0.095 -0.671 -0.828 -1.0  \n",
      "2    -1.000 -1.000 -1.000 -0.109  1.000 -0.179 -1.000 -1.000 -1.000 -1.0  \n",
      "3    -0.318  1.000  0.536 -0.987 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "4     0.466  0.639  1.000  1.000  0.791  0.439 -0.199 -0.883 -1.000 -1.0  \n",
      "...     ...    ...    ...    ...    ...    ...    ...    ...    ...  ...  \n",
      "7286 -0.116  0.899  0.416 -0.510 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "7287  0.697  0.636  0.167 -0.968 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "7288  0.805  1.000  1.000  0.727 -0.342 -0.933 -1.000 -1.000 -1.000 -1.0  \n",
      "7289 -0.231  0.621  0.999 -0.042 -0.231 -0.687 -1.000 -1.000 -1.000 -1.0  \n",
      "7290 -0.634  0.803  0.589 -0.907 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "\n",
      "[7291 rows x 257 columns]\n"
     ]
    }
   ],
   "source": [
    "#train on USPS\n",
    "#label data as -1 iff digit <=4, else 1\n",
    "df_usps_modified = df_usps.copy()\n",
    "for i in range(df_usps_modified.shape[0]):\n",
    "    if df_usps_modified.loc[i][0] <=4:\n",
    "        df_usps_modified.loc[i][0] = -1\n",
    "    else:\n",
    "        df_usps_modified.loc[i][0] = 1\n",
    "        \n",
    "print(df_usps_modified)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0    1      2      3      4      5      6      7      8      9    ...  \\\n",
      "5342 -1.0 -1.0 -0.195  1.000  0.448  0.192  0.529  0.529  0.529  0.529  ...   \n",
      "177  -1.0 -1.0 -1.000 -1.000 -0.847  0.609  1.000  0.604 -0.529 -1.000  ...   \n",
      "1772 -1.0 -1.0 -0.264  0.372  0.372  0.372  0.977  0.555  0.314 -0.255  ...   \n",
      "6286 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -0.441  0.999  0.999  0.049  ...   \n",
      "4833 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.129  0.845  ...   \n",
      "...   ...  ...    ...    ...    ...    ...    ...    ...    ...    ...  ...   \n",
      "4936  1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -0.919 -0.063  0.980  1.000  ...   \n",
      "3501  1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -0.837 -0.125  0.462  ...   \n",
      "3821 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000  0.399  0.930 -0.133 -0.334  ...   \n",
      "4675  1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -0.996 -0.597  0.032  0.551  ...   \n",
      "3546 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -0.998 -0.562  0.113  0.793  ...   \n",
      "\n",
      "        247    248    249    250    251    252    253    254    255   256  \n",
      "5342  0.059  0.470  0.850  1.000  0.845 -0.282 -0.997 -1.000 -1.000 -1.00  \n",
      "177  -0.802 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.00  \n",
      "1772 -0.959 -1.000 -1.000 -1.000 -1.000 -0.963 -0.665 -0.312 -0.940 -1.00  \n",
      "6286  0.999  0.999  0.999  0.954 -0.214 -1.000 -1.000 -1.000 -1.000 -1.00  \n",
      "4833 -1.000  0.610  0.947 -0.676 -1.000 -1.000 -1.000 -1.000 -1.000 -1.00  \n",
      "...     ...    ...    ...    ...    ...    ...    ...    ...    ...   ...  \n",
      "4936  0.538  1.000  0.591  0.201 -0.067 -0.577 -0.992 -1.000 -1.000 -1.00  \n",
      "3501 -1.000 -1.000 -0.669  0.727 -0.558 -1.000 -1.000 -1.000 -1.000 -1.00  \n",
      "3821 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.050  1.000  0.636 -0.92  \n",
      "4675 -0.962  0.262  0.735 -0.825 -1.000 -1.000 -1.000 -1.000 -1.000 -1.00  \n",
      "3546  1.000  0.390  0.219 -0.157 -0.883 -1.000 -1.000 -1.000 -1.000 -1.00  \n",
      "\n",
      "[2000 rows x 257 columns]\n"
     ]
    }
   ],
   "source": [
    "seed_value=10\n",
    "#sample 2000 from them\n",
    "df_usps_sample = df_usps_modified.sample(2000,random_state=seed_value)\n",
    "\n",
    "print(df_usps_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method of partitioning data fram in 50*40\n",
    "def partition_dataframe(df):\n",
    "    result=[]\n",
    "    df_to_sample_from = df.copy()\n",
    "    for i in range(0,50):\n",
    "        train = df_to_sample_from.sample(40,random_state=seed_value)\n",
    "        remaining = df_to_sample_from.drop(train.index)\n",
    "        df_to_sample_from = remaining\n",
    "        result.append(train)\n",
    "    return result\n",
    "partition = partition_dataframe(df_usps_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0    1      2      3      4      5      6      7      8      9    ...  \\\n",
      "6353  1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -0.676  0.666 -0.327  ...   \n",
      "2179 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -0.506  1.000  0.790 -0.643  ...   \n",
      "2314 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -0.936  0.822  0.410  ...   \n",
      "6562 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -0.462  0.878  0.032  ...   \n",
      "3710  1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -0.800 -0.239  0.254  ...   \n",
      "5183 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.809  0.200  ...   \n",
      "5403  1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -0.966 -0.450  0.045  ...   \n",
      "6614 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -0.484  0.640  1.000  0.918  ...   \n",
      "85    1.0 -1.0 -0.968 -0.428  0.511  1.000  1.000  1.000  1.000  0.923  ...   \n",
      "2611 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  ...   \n",
      "6633 -1.0 -1.0 -1.000 -1.000 -1.000 -0.944 -0.008  0.333  0.929  1.000  ...   \n",
      "4705  1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.317  0.002  ...   \n",
      "6774  1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -0.525  0.888  0.900  ...   \n",
      "1972 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -0.360  0.997  0.542  ...   \n",
      "4710 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -0.142  0.111  0.838  1.000  ...   \n",
      "3048  1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  ...   \n",
      "970   1.0 -1.0 -1.000 -0.934  0.239 -0.017 -0.334  0.016  0.333  0.522  ...   \n",
      "5498 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  ...   \n",
      "4152 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  0.181 -0.368  ...   \n",
      "3137 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  ...   \n",
      "2089  1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.539  ...   \n",
      "5051  1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  ...   \n",
      "5010 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.664  0.395  ...   \n",
      "6956 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  0.257 -0.354  ...   \n",
      "1029  1.0 -1.0 -1.000 -1.000 -0.588  0.461  0.993  0.746  0.284 -0.574  ...   \n",
      "1011  1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -0.981  0.058  0.952  ...   \n",
      "2008  1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.954 -0.642  ...   \n",
      "1654 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -0.841  0.362  0.849  ...   \n",
      "2986 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.999  0.500  ...   \n",
      "835  -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  0.213  0.727  ...   \n",
      "420   1.0 -1.0 -1.000 -0.968 -0.595 -0.392 -0.363  0.072  0.192  0.536  ...   \n",
      "7251  1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -0.934  0.423  0.834 -0.196  ...   \n",
      "2279  1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -0.861 -0.362  0.081  ...   \n",
      "2812 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -0.379  0.659 -0.702  ...   \n",
      "885  -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -0.940  0.628  0.778  0.171  ...   \n",
      "6595 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  ...   \n",
      "3896  1.0 -1.0 -1.000 -1.000 -1.000 -0.810  0.514  1.000  1.000  0.383  ...   \n",
      "4884 -1.0 -1.0 -1.000 -1.000 -1.000 -0.783 -0.269  0.172  0.040 -0.053  ...   \n",
      "6590 -1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -0.982  0.664  0.393 -0.861  ...   \n",
      "4114  1.0 -1.0 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.087  ...   \n",
      "\n",
      "        247    248    249    250    251    252    253    254    255  256  \n",
      "6353  0.081  0.484  0.956  0.677 -0.576 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "2179  1.000  1.000  0.975  0.111 -0.131 -0.866 -1.000 -1.000 -1.000 -1.0  \n",
      "2314 -1.000 -0.826  0.302 -0.004 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "6562  0.728  1.000  1.000  0.718 -0.230 -0.989 -1.000 -1.000 -1.000 -1.0  \n",
      "3710 -1.000 -1.000  0.446  0.468 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "5183  0.247  0.718  0.795  0.418 -0.165 -0.951 -1.000 -1.000 -1.000 -1.0  \n",
      "5403  0.462  1.000  1.000  1.000 -0.266 -0.977 -1.000 -1.000 -1.000 -1.0  \n",
      "6614  0.392  0.639  0.179 -0.270 -0.796 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "85   -0.512  0.287  0.832  1.000 -0.097 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "2611 -1.000 -0.952  0.531  0.280 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "6633  0.468  1.000  1.000  0.643 -0.071 -0.786 -1.000 -1.000 -1.000 -1.0  \n",
      "4705  0.453 -0.359 -0.382 -0.937 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "6774  0.393  1.000  0.899  0.255 -0.917 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "1972  0.999  0.999  0.527 -0.824 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "4710  0.111 -0.471 -0.906 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "3048  0.055  0.798  1.000  1.000  1.000  0.457 -0.566 -1.000 -1.000 -1.0  \n",
      "970   0.414  0.252 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "5498 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "4152 -1.000  0.913 -0.133 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "3137 -1.000 -1.000 -1.000 -1.000 -1.000  0.528 -0.097 -1.000 -1.000 -1.0  \n",
      "2089 -0.534  0.841  0.549 -0.059 -0.936 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "5051 -0.998 -0.996 -0.996 -0.996 -0.996 -0.999 -1.000 -1.000 -1.000 -1.0  \n",
      "5010 -1.000  0.420  0.369 -0.985 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "6956 -1.000  0.073  0.484 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "1029  0.596  0.768 -0.635 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "1011 -0.216  0.579  0.762 -0.385 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "2008 -0.232  0.372  0.815  0.520 -0.180 -0.968 -1.000 -1.000 -1.000 -1.0  \n",
      "1654  0.480  1.000  1.000  0.607 -0.483 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "2986 -1.000 -0.418  0.657 -0.993 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "835   1.000  1.000  0.985  0.301 -0.632 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "420   0.393  0.656  0.014 -0.977 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "7251  0.511  1.000  0.727 -0.452 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "2279 -1.000 -1.000 -0.099  0.951  0.320 -0.032 -0.970 -1.000 -1.000 -1.0  \n",
      "2812  0.903  1.000  0.990  0.224 -0.976 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "885   0.384  0.824  0.702  0.362 -0.355 -0.992 -1.000 -1.000 -1.000 -1.0  \n",
      "6595 -1.000 -1.000 -0.122  0.946 -0.298 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "3896  0.308  0.934  1.000  1.000  0.600  0.083 -0.729 -1.000 -1.000 -1.0  \n",
      "4884  0.511  0.183 -0.053 -0.053 -0.053 -0.053 -0.128 -0.617 -0.854 -1.0  \n",
      "6590  0.513  0.827 -0.168 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "4114  0.497 -0.931 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.0  \n",
      "\n",
      "[40 rows x 257 columns]\n",
      "0.7877551020408163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"svm = SVM(kernel='linear')\\nsvm.fit_on_data(train_data,train_targets,C=1)\\nprint(svm.accuracy(test_data,test_targets))\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(partition[0])\n",
    "#this is just testing the first case (where we take the first 40 as train)\n",
    "#Just to see accuracy\n",
    "train_data = partition[0].iloc[:,1:].to_numpy()\n",
    "train_targets = partition[0].iloc[:,0].to_numpy()\n",
    "df_test = df_usps_sample.drop(partition[0].index)\n",
    "test_data = df_test.iloc[:,1:].to_numpy()\n",
    "test_targets = df_test.iloc[:,0].to_numpy()\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='rbf',C=10,gamma='scale')\n",
    "clf.fit(train_data, train_targets)\n",
    "predicted = clf.predict(test_data)\n",
    "N = test_targets.shape[0]\n",
    "correct = 0\n",
    "for j in range(N):\n",
    "    if predicted[j] == test_targets[j]:\n",
    "        correct+=1\n",
    "print(correct/N)\n",
    "\n",
    "'''svm = SVM(kernel='linear')\n",
    "svm.fit_on_data(train_data,train_targets,C=1)\n",
    "print(svm.accuracy(test_data,test_targets))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7877551  0.4377551  0.49387755 0.45408163 0.47755102 0.48112245\n",
      " 0.48010204 0.57244898 0.4994898  0.52040816 0.42244898 0.53418367\n",
      " 0.43469388 0.5122449  0.58112245 0.45510204 0.55306122 0.51683673\n",
      " 0.55561224 0.51530612 0.47193878 0.44234694 0.51734694 0.5494898\n",
      " 0.4244898  0.58673469 0.47193878 0.4627551  0.58877551 0.44540816\n",
      " 0.47040816 0.48928571 0.38214286 0.48877551 0.58316327 0.45612245\n",
      " 0.48163265 0.40204082 0.52193878 0.52040816 0.55765306 0.65\n",
      " 0.5872449  0.47244898 0.50459184 0.56173469 0.50306122 0.57908163\n",
      " 0.56632653 0.60459184]\n",
      "mean: 0.5125816326530612\n",
      "std: 0.06955297032453944\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.zeros(50)\n",
    "from sklearn import svm\n",
    "#do the partition\n",
    "for i in range(50):\n",
    "    #for each i, use partition[i] as train, and the rest as test\n",
    "    data_partition = partition[i].iloc[:,1:]\n",
    "    targets = partition[0].iloc[:,0].to_numpy()\n",
    "    inputs =  data_partition.to_numpy()\n",
    "    test_df = df_usps_sample.drop(partition[i].index) #'drop' simply ingores the indeces from particion [i], giving us the 'rest'\n",
    "    test_targets=test_df.iloc[:,0].to_numpy()\n",
    "    test_inputs =test_df.iloc[:,1:].to_numpy()\n",
    "    #svm = SVM(kernel='linear')\n",
    "    clf = svm.SVC(kernel='rbf', C=10,gamma='scale')\n",
    "    clf.fit(inputs, targets)\n",
    "    #print(test_inputs)\n",
    "    #svm.fit_on_data(inputs,targets)\n",
    "    #accuracy[i] = svm.accuracy(test_inputs,test_targets)\n",
    "    predicted = clf.predict(test_inputs)\n",
    "    N = test_targets.shape[0]\n",
    "    correct = 0\n",
    "    for j in range(N):\n",
    "        if predicted[j] == test_targets[j]:\n",
    "            correct+=1\n",
    "    accuracy[i]= correct/N\n",
    "print(accuracy)\n",
    "print('mean:', np.mean(accuracy))\n",
    "print('std:', np.std(accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
