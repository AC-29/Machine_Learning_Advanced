{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dumit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer,TfidfVectorizer\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "['comp.sys.mac.hardware', 'comp.windows.x']\n",
      "['comp.sys.mac.hardware', 'comp.windows.x']\n",
      "[ 1  1  1 ... -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "categories = ['comp.sys.mac.hardware','comp.windows.x']\n",
    "stop_words = list(stopwords.words('english')) \n",
    "print(stop_words)\n",
    "\n",
    "#categories = ['comp.sys.mac.hardware','comp.os.ms-windows.misc']\n",
    "train_data = fetch_20newsgroups(subset='train',categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "train_inputs = train_data.data\n",
    "train_targets = train_data.target\n",
    "train_targets[train_targets==0]=-1\n",
    "print(train_data.target_names)\n",
    "test_data = fetch_20newsgroups(subset='test',categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "test_inputs = test_data.data\n",
    "test_targets = test_data.target\n",
    "test_targets[test_targets==0]=-1\n",
    "print(test_data.target_names)\n",
    "print(train_targets)\n",
    "#mac = 1\n",
    "mac=1\n",
    "#windows = 0\n",
    "windows=-1\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words,use_idf=True)\n",
    "vectorizer = vectorizer.fit(train_inputs)\n",
    "\n",
    "transformed_train_inputs = vectorizer.transform(train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17567342,)\n",
      "7491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(last_words.shape)\\n\\nlast_words_list = []\\n\\nfor i in last_words:\\n    last_words_list.append(i)\\n\\nprint(len(last_words_list))\\n\\n#voc = vectorizer.vocabulary_\\n\\n\\nprint(tfidf_sorting)\\n\\nprint(last_words)\\n\\nstop_words_new = last_words_list + stop_words\\n\\n#print(stop_words_new)'"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "feature_array = np.array(vectorizer.get_feature_names())\n",
    "tfidf_sorting = np.argsort(transformed_train_inputs.toarray()).flatten()[::-1]\n",
    "\n",
    "\n",
    "print(tfidf_sorting.shape)\n",
    "\n",
    "n = 7511\n",
    "top_words =  feature_array[tfidf_sorting][:n]\n",
    "\n",
    "voc = vectorizer.vocabulary_\n",
    "\n",
    "last_words =[]\n",
    "\n",
    "for i in voc.keys():\n",
    "    if (i not in top_words):\n",
    "        last_words.append(i)\n",
    "\n",
    "\n",
    "print(len(last_words))\n",
    "\n",
    "'''\n",
    "print(last_words.shape)\n",
    "\n",
    "last_words_list = []\n",
    "\n",
    "for i in last_words:\n",
    "    last_words_list.append(i)\n",
    "\n",
    "print(len(last_words_list))\n",
    "\n",
    "#voc = vectorizer.vocabulary_\n",
    "\n",
    "\n",
    "print(tfidf_sorting)\n",
    "\n",
    "print(last_words)\n",
    "\n",
    "stop_words_new = last_words_list + stop_words\n",
    "\n",
    "#print(stop_words_new)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.32708941 3.74727091 6.96614674 ... 2.60092722 2.55537069 7.37161185]\n",
      "(7511,)\n"
     ]
    }
   ],
   "source": [
    "stop_words_new = last_words+ stop_words\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words_new,use_idf=True)\n",
    "vectorizer= vectorizer.fit(train_inputs)\n",
    "\n",
    "transformed_train_inputs = vectorizer.transform(train_inputs)\n",
    "\n",
    "idf = vectorizer_new.idf_\n",
    "\n",
    "print(idf)\n",
    "\n",
    "print(idf.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test (780, 7511)\n",
      "[7.37331979 7.37331979 7.37331979 7.37331979 7.37331979 7.37331979\n",
      " 7.37331979 7.37331979 7.37331979 7.37331979]\n",
      "(7511,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "# Extract features used tf-idf mapping\n",
    "#vectorizer = TfidfVectorizer(stop_words=stop_words,use_idf=True)\n",
    "#vectorizer = TfidfVectorizer(use_idf=True,smooth_idf=True)\n",
    "'''vectorizer = vectorizer.fit(train_inputs)\n",
    "transformed_train_inputs = vectorizer.transform(train_inputs)'''\n",
    "#count_vectorizer = CountVectorizer(max_features=7511) #as explained in the paper\n",
    "#train_inputs = vectorizer.fit_transform(train_inputs)\n",
    "transformed_test_inputs = vectorizer.transform(test_inputs)\n",
    "'''tfidf_transformer = TfidfTransformer()\n",
    "train_inputs = tfidf_transformer.fit_transform(train_inputs)\n",
    "test_inputs = tfidf_transformer.transform(test_inputs)\n",
    "'''\n",
    "print('test', transformed_test_inputs.shape)\n",
    "a =np.sort(vectorizer.idf_)\n",
    "print(a[-10:])\n",
    "print(vectorizer.idf_.shape)\n",
    "#print(vectorizer.vocabulary_)\n",
    "df = pd.DataFrame(transformed_train_inputs.todense(), columns=vectorizer.get_feature_names())\n",
    "df = df.assign(Labels=pd.Series(train_targets).values)\n",
    "df.drop_duplicates(keep='first',inplace=True)\n",
    "\n",
    "\n",
    "df_test = pd.DataFrame(transformed_test_inputs.todense(), columns=vectorizer.get_feature_names())\n",
    "df_test = df_test.assign(Labels=pd.Series(test_targets).values)\n",
    "df_test.drop_duplicates(keep='first',inplace=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 000         0.000604\n",
      "0000        0.000071\n",
      "00000000    0.000200\n",
      "00000074    0.000401\n",
      "00000093    0.000116\n",
      "              ...   \n",
      "would       0.027527\n",
      "year        0.004455\n",
      "yet         0.004041\n",
      "zyxel       0.000511\n",
      "Labels      0.024867\n",
      "Length: 7512, dtype: float64\n",
      "train std 000         0.010466\n",
      "0000        0.002374\n",
      "00000000    0.006721\n",
      "00000074    0.013442\n",
      "00000093    0.003884\n",
      "              ...   \n",
      "would       0.062225\n",
      "year        0.029858\n",
      "yet         0.026919\n",
      "zyxel       0.012136\n",
      "Labels      1.000135\n",
      "Length: 7512, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "'''print(transformed_train_inputs.shape)\n",
    "print(transformed_test_inputs.shape)\n",
    "print(transformed_train_inputs.shape[0] + transformed_test_inputs.shape[0])\n",
    "print('windows:',transformed_train_inputs[train_targets == windows].shape[0] + transformed_test_inputs[test_targets == windows].shape[0])\n",
    "print('mac:', transformed_train_inputs[train_targets == mac].shape[0] + transformed_test_inputs[test_targets == mac].shape[0])\n",
    "'''\n",
    "\n",
    "print('train', df.mean())\n",
    "print('train std', df.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mac 955\n",
      "windows 921\n"
     ]
    }
   ],
   "source": [
    "train_targets = df['Labels'].to_numpy()\n",
    "\n",
    "test_targets= df_test['Labels'].to_numpy()\n",
    "\n",
    "print('mac', train_targets[train_targets==mac].shape[0]+test_targets[test_targets==mac].shape[0])\n",
    "\n",
    "print('windows',train_targets[train_targets==windows].shape[0]+test_targets[test_targets==windows].shape[0])\n",
    "\n",
    "train_w = np.where(train_targets==windows)[0]\n",
    "train_m = np.where(train_targets==windows)[0]\n",
    "\n",
    "#data_w = transformed_train_inputs[train_targets==windows]\n",
    "#data_m = transformed_train_inputs[train_targets==mac]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "#pick 128 as your sample\n",
    "seed_value=1\n",
    "np.random.seed(seed_value)\n",
    "random_index_w =np.random.choice(train_w.shape[0], 64, replace=False) \n",
    "'''train_w = train_w[random_index_w]\n",
    "data_w = data_w[random_index_w]'''\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "random_index_m =np.random.choice(train_m.shape[0], 64, replace=False)\n",
    "'''train_m = train_m[random_index_m]\n",
    "data_m = data_m[random_index_m]'''\n",
    "#print(random_index)\n",
    "random.seed(seed_value)\n",
    "#train_size = random.randint(2,129)\n",
    "train_size = 8\n",
    "'''print(data_w[0])\n",
    "print(train_w.shape)\n",
    "print(train_m.shape)\n",
    "print(train_size)'''\n",
    "\n",
    "unlabeled_size = int(random_index_w.shape[0] - train_size)\n",
    "print(unlabeled_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick sample 16 training data as 8 and 8 from each dataset\n",
    "seed_value=1\n",
    "np.random.seed(seed_value)\n",
    "random_index_w_labeled =np.random.choice(random_index_w, train_size, replace=False)\n",
    "#labeled_w = train_w[random_index_w_labeled]\n",
    "#labeled_data_w = data_w[random_index_w_labeled]\n",
    "\n",
    "random_index_w_unlabeled = []\n",
    "for i in random_index_w:\n",
    "    if (i not in random_index_w_labeled):\n",
    "        random_index_w_unlabeled.append(i)\n",
    "random_index_w_unlabeled = np.array(random_index_w_unlabeled)\n",
    "\n",
    "\n",
    "random_index_m_labeled =np.random.choice(random_index_m, train_size, replace=False)\n",
    "#labeled_m = train_m[random_index_w_labeled]\n",
    "#labeled_data_m = data_m[random_index_w_labeled]\n",
    "\n",
    "random_index_m_unlabeled = []\n",
    "for i in random_index_m:\n",
    "    if (i not in random_index_m_labeled):\n",
    "        random_index_m_unlabeled.append(i)\n",
    "random_index_m_unlabeled = np.array(random_index_w_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled windows (8,)\n",
      "labeled mac (8,)\n",
      "unlabeled windows (56,)\n",
      "unlabeled mac (56,)\n",
      "labeled total (16,)\n",
      "unlabeled total (112,)\n"
     ]
    }
   ],
   "source": [
    "labeled_train = np.concatenate((random_index_w_labeled,random_index_m_labeled))\n",
    "unlabeled_train = np.concatenate((random_index_w_unlabeled,random_index_m_unlabeled))\n",
    "\n",
    "\n",
    "\n",
    "print('labeled windows', random_index_w_labeled.shape)\n",
    "print('labeled mac', random_index_m_labeled.shape)\n",
    "\n",
    "print('unlabeled windows', random_index_w_unlabeled.shape)\n",
    "print('unlabeled mac', random_index_m_unlabeled.shape)\n",
    "\n",
    "print('labeled total', labeled_train.shape)\n",
    "print('unlabeled total', unlabeled_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets write this all in a nice way\n",
    "\n",
    "def split_20news_data(data,targets,seed):\n",
    "    #pick 64 for train from each data set\n",
    "    train_w = np.where(targets==windows)[0]\n",
    "    train_m = np.where(targets==mac)[0]\n",
    "    random_index_w =np.random.choice(train_w, 64, replace=False) \n",
    "    random_index_m =np.random.choice(train_m, 64, replace=False)\n",
    "    train_size = 8 #8 from each type of document\n",
    "    #unlabeled = train - train_labeled\n",
    "    unlabeled_size = int(random_index_w.shape[0] - train_size)\n",
    "    #pick the labeled now\n",
    "    random_index_w_labeled =np.random.choice(random_index_w, train_size, replace=False)\n",
    "    '''labeled_w = train_w[random_index_w_labeled]\n",
    "    labeled_data_w = data_w[random_index_w_labeled]'''\n",
    "    #unlabeled_w = train_w - labeled_w\n",
    "    random_index_w_unlabeled = []\n",
    "    for i in random_index_w:\n",
    "        if (i not in random_index_w_labeled):\n",
    "            random_index_w_unlabeled.append(i)\n",
    "    random_index_w_unlabeled = np.array(random_index_w_unlabeled)\n",
    "\n",
    "    #do the same for m\n",
    "    random_index_m_labeled =np.random.choice(random_index_m, train_size, replace=False)\n",
    "    '''labeled_m = train_m[random_index_w_labeled]\n",
    "    labeled_data_m = data_m[random_index_w_labeled]'''\n",
    "    #print(targets[random_index_m_labeled])\n",
    "    random_index_m_unlabeled = []\n",
    "    for i in random_index_m:\n",
    "        if (i not in random_index_m_labeled):\n",
    "            random_index_m_unlabeled.append(i)\n",
    "    random_index_m_unlabeled = np.array(random_index_m_unlabeled)\n",
    "    #lastly, labeled train = labeled_m + labeled_w and similarly unlabeled\n",
    "    labeled_train = np.concatenate((random_index_w_labeled,random_index_m_labeled))\n",
    "    unlabeled_train = np.concatenate((random_index_w_unlabeled,random_index_m_unlabeled))\n",
    "    df_labeled = pd.DataFrame(df.iloc[labeled_train])\n",
    "    df_unlabeled = pd.DataFrame(df.iloc[unlabeled_train])\n",
    "    return df_labeled, df_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn import svm'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''labeled_data = np.concatenate((labeled_data_w,labeled_data_m))\n",
    "tr_targets = np.concagenate((labeled_w,labeled_m))'''\n",
    "\n",
    "label,unlabel = split_20news_data(transformed_train_inputs,train_targets,1)\n",
    "'''from sklearn import svm'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_file = [label,unlabel, df_test]\n",
    "import pickle\n",
    "with open('20news_databases', 'wb') as fp:\n",
    "    pickle.dump(dataframe_file, fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00000000</th>\n",
       "      <th>00000074</th>\n",
       "      <th>00000093</th>\n",
       "      <th>000000e5</th>\n",
       "      <th>000005102000</th>\n",
       "      <th>00000510200001</th>\n",
       "      <th>00000ee5</th>\n",
       "      <th>000010af</th>\n",
       "      <th>...</th>\n",
       "      <th>week</th>\n",
       "      <th>width</th>\n",
       "      <th>winner</th>\n",
       "      <th>world</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "      <th>zyxel</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.067525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.256945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 7512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      000  0000  00000000  00000074  00000093  000000e5  000005102000  \\\n",
       "976   0.0   0.0       0.0       0.0       0.0       0.0           0.0   \n",
       "1111  0.0   0.0       0.0       0.0       0.0       0.0           0.0   \n",
       "910   0.0   0.0       0.0       0.0       0.0       0.0           0.0   \n",
       "1010  0.0   0.0       0.0       0.0       0.0       0.0           0.0   \n",
       "1145  0.0   0.0       0.0       0.0       0.0       0.0           0.0   \n",
       "...   ...   ...       ...       ...       ...       ...           ...   \n",
       "144   0.0   0.0       0.0       0.0       0.0       0.0           0.0   \n",
       "1154  0.0   0.0       0.0       0.0       0.0       0.0           0.0   \n",
       "576   0.0   0.0       0.0       0.0       0.0       0.0           0.0   \n",
       "374   0.0   0.0       0.0       0.0       0.0       0.0           0.0   \n",
       "1066  0.0   0.0       0.0       0.0       0.0       0.0           0.0   \n",
       "\n",
       "      00000510200001  00000ee5  000010af  ...  week     width  winner  world  \\\n",
       "976              0.0       0.0       0.0  ...   0.0  0.000000     0.0    0.0   \n",
       "1111             0.0       0.0       0.0  ...   0.0  0.000000     0.0    0.0   \n",
       "910              0.0       0.0       0.0  ...   0.0  0.000000     0.0    0.0   \n",
       "1010             0.0       0.0       0.0  ...   0.0  0.000000     0.0    0.0   \n",
       "1145             0.0       0.0       0.0  ...   0.0  0.000000     0.0    0.0   \n",
       "...              ...       ...       ...  ...   ...       ...     ...    ...   \n",
       "144              0.0       0.0       0.0  ...   0.0  0.000000     0.0    0.0   \n",
       "1154             0.0       0.0       0.0  ...   0.0  0.000000     0.0    0.0   \n",
       "576              0.0       0.0       0.0  ...   0.0  0.000000     0.0    0.0   \n",
       "374              0.0       0.0       0.0  ...   0.0  0.000000     0.0    0.0   \n",
       "1066             0.0       0.0       0.0  ...   0.0  0.256945     0.0    0.0   \n",
       "\n",
       "         worth  would  year  yet  zyxel  Labels  \n",
       "976   0.000000    0.0   0.0  0.0    0.0      -1  \n",
       "1111  0.067525    0.0   0.0  0.0    0.0      -1  \n",
       "910   0.000000    0.0   0.0  0.0    0.0      -1  \n",
       "1010  0.000000    0.0   0.0  0.0    0.0      -1  \n",
       "1145  0.000000    0.0   0.0  0.0    0.0      -1  \n",
       "...        ...    ...   ...  ...    ...     ...  \n",
       "144   0.000000    0.0   0.0  0.0    0.0       1  \n",
       "1154  0.000000    0.0   0.0  0.0    0.0       1  \n",
       "576   0.000000    0.0   0.0  0.0    0.0       1  \n",
       "374   0.000000    0.0   0.0  0.0    0.0       1  \n",
       "1066  0.000000    0.0   0.0  0.0    0.0       1  \n",
       "\n",
       "[112 rows x 7512 columns]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
